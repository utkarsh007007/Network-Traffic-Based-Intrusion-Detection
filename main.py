# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D35qVXpvb5-ZYmmYuklysH5Szmlj19Ti
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import train_test_split
from tabulate import tabulate
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.svm import SVC

train = pd.read_csv("/content/Train_data.csv")
test  = pd.read_csv("/content/Test_data.csv")

train.info()

test.info()

train["class"].value_counts()

sns.countplot(x=train['class'])

cat=train.select_dtypes(include="O")
for col in cat:
    print(train[col].value_counts())

def encode(df):
    for col in df.columns:
        if df[col].dtype == 'object':
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])

encode(train)
encode(test)

duplicate_cols = []
for i in range(len(train.columns)):
    for j in range(i+1, len(train.columns)):
        if train[train.columns[i]].equals(train[train.columns[j]]):
            duplicate_cols.append((train.columns[i], train.columns[j]))
print("Duplicate columns:", duplicate_cols)

train.drop(['num_outbound_cmds'],axis=1,inplace=True)
test.drop(['num_outbound_cmds'],axis=1,inplace=True)

X = train.drop(['class'], axis=1)
y = train['class']


rfc = RandomForestClassifier()
rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(X, y)

selected_features = [v for i, v in itertools.zip_longest(rfe.get_support(), X.columns) if i==True]
selected_features

X = X[selected_features]
test = test[selected_features]

x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)

scale = StandardScaler()
x_train = scale.fit_transform(x_train)
x_test = scale.transform(x_test)
test_scaled = scale.transform(test)

def show_summary(models, x_train, x_test, y_train, y_test):
    data = []
    for name, model in models.items():
        model.fit(x_train, y_train)
        train_score = model.score(x_train, y_train)
        test_score = model.score(x_test, y_test)
        data.append([name, train_score, test_score])
    print(tabulate(data, headers=["Model", "Train Score", "Test Score"], tablefmt="fancy_grid"))

models_selected = {
    "Random Forest": RandomForestClassifier(max_depth=11, max_features=6, n_estimators=12, random_state=42),
    "Decision Tree": DecisionTreeClassifier(max_depth=10, max_features=6, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "XGBoost": XGBClassifier(objective="binary:logistic", random_state=42, n_estimators=300, max_depth=8, learning_rate=0.1),
    "CatBoost": CatBoostClassifier(verbose=0),
    "SVM": SVC(kernel='rbf', C=1.0)
}

show_summary(models_selected, x_train, x_test, y_train, y_test)

voting_for_models_selected = VotingClassifier(estimators=[
    ('RF', models_selected['Random Forest']),
    ('DT', models_selected['Decision Tree']),
    ('GB', models_selected['Gradient Boosting']),
    ('XGB', models_selected['XGBoost']),
    ('CB', models_selected['CatBoost']),
    ('SVM', models_selected['SVM']) ], voting='hard') #can't use soft voting for SVM

voting_for_models_selected.fit(x_train, y_train)
print("Voting Train Score:", voting_for_models_selected.score(x_train, y_train))
print("Voting Test Score:", voting_for_models_selected.score(x_test, y_test))